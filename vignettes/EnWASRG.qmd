---
title: 'EnWAS Meta-Analysis: Systolic BP'
format:
  html:
    code-fold: true
fig-width: 10
fig-height: 8
fig-format: svg
cache: false
---

# Introduction

The notion of an environment-wide association study (EnWAS) is to ascertain the effect of a number of features of interest on an outcome of interest. Since in epidemiology there are often many features and exposures that might be well known to be associated with the outcome of interest we will often want to adjust for some subset of those before assessing the set of EnWAS variables.  One specific point is that EnWAS is not trying to fit an optimal model, per se. It is trying to inform as to the effects of the EnWAS variables on an outcome that has already been adjusted for a number of important and known risk factors. 
  The basic ideas are modeled on the the genome-wide association studies (GWAS) that are widely used in genetics to try to understand the univariate effect of single nucleotide polymorphisms (SNPs) on different outcomes, such as disease risk or phenotype.  These have been quite successful and replicable across a wide variety of studies. We hope that there are situations in epidemiology where we may also find effects that are replicable and of scientific relevance for the outcomes and features being considered.
  NHANES (cite NHANES) provides a somewhat ideal resource for developing methodology and exploring how these ideas might transfer over to the area of epidemiology. The fact that NHANES consists of multiple independent cycles provides some opportunities to explore notions of replicability and consistency of models and predictions. 
   In this paper we outline the process using a specific example of systolic blood pressure and the effects of different nutritional intakes. We believe that this will provide sufficient evidence of the value of the approach and perhaps motivate others to extend and enhance these efforts.
  
 
## Methods

Our broad strategy is the following:

* Choose an outcome variable: In this case, systolic blood pressure.

* Identify a set of confounders that are known to affect the outcome, but which are not themselves of particular interst in this analysis.  These will be used to identify and fit a baseline model.

* Choose a set of phenotypes to see which, if any, are associated with
  the outcome. For NHANES, a natural choice is the set of nutrition
  variables.

In our example we consider the the following features that are known
to associate with changes in blood pressure. 

TODO: how do we think about the relationship between these and choices in food.

* Age: Increases with age

* Sex: Men tend to have higher BP until women pass menopause then it reverses

* Race: Black/African American individuals often have higher BP

* BMI: Strong positive association

* Socioeconomic Status (SES): Lower SES is linked with higher BP

* Education level: Lower education is associated with higher BP

In this example, we are interested in finding NUTRITION PHENOTYPES
that associate with systolic blood pressure. Some choices we make for
the analysis:

### Concerns about confounding.

One of the major challenges in epidemiology, and indeed in most data science applications, is the issue of confounding.  In GWAS studies this can be less important since the genome is essentially determined at birth and not that maleable. However, it is certainly the case that variants in the genome do affect phenotypes, that is the whole purpose of using GWAS, however situations where one SNP has a big impact tend to be rare.  What is more common is that population structure can induce strong signals.  If one consider red hair as a phenotype of interest, then any study that included individuals from Scotland, where the prevalence is about 13% (about 10x global prevalence), would need to address populations structure since any SNP that was higher prevalence in the Scottish population would associate with the red hair phenotype.
  One approach that seems helpful in the EnWAS context would be to use a machine learning method to try to understand the relationships between the risk factors included in the base-line model and the set of EnWAS variables.
  


### Examples


A detailed analysis for one cycle is available
[here](EnWAS-SysBP-single-cycle.html). In this document, we follow up
on that analysis by fixing a baseline model and phenotypes of interest
at the very beginning, and replicate the analysis for each cycle
separately.

```{r, echo=FALSE, message=FALSE}
library(nhanesA)
library(phonto)
library(PheWAS)
library(splines)
library(lattice)
library(latticeExtra)
library(metafor)
library(randomForest)
library(forestplot)
```

# Baseline model and exposure variables

```{r setupBaseline}
base_model <- 'SYSTOLIC ~ ns(RIDAGEYR, df = 5) * RIAGENDR + race + obese + DMDEDUC2 + lowincome'
exposure_vars <-
    c("SEQN", "DR1DRSTZ", "DRDINT", "DRQSPREP","DR1TNUMF", "DR1TKCAL", "DR1TPROT",
      "DR1TCARB", "DR1TSUGR", "DR1TFIBE", "DR1TTFAT", "DR1TSFAT", "DR1TMFAT",
      "DR1TPFAT", "DR1TCHOL", "DR1TATOC", "DR1TRET"  ,"DR1TVARA", "DR1TBCAR",
      "DR1TCRYP", "DR1TLZ", "DR1TVB1" , "DR1TVB2" , "DR1TNIAC", "DR1TVB6" ,
      "DR1TFOLA", "DR1TFA", "DR1TFF", "DR1TFDFE", "DR1TVB12", "DR1TVC","DR1TVK",
      "DR1TCALC", "DR1TPHOS", "DR1TMAGN", "DR1TIRON", "DR1TZINC", "DR1TCOPP", "DR1TSODI",
      "DR1TPOTA", "DR1TSELE", "DR1TMOIS", "DR1TS040","DR1TS080", "DR1TS120",
      "DR1TS160", "DR1TS180", "DR1TM161", "DR1TM181", "DR1TM201", "DR1TP183",
      "DR1TP204")
```

# Per cycle reports

We will do EnWAS analysis separately for each cycle, and then try to compare.

`DR1TOT` is only available from cycle C onwards, so we start from there.

```{r}
nhanesSearchTableNames("DR1TOT")
```


## Utility functions to extract data for a cycle

This needs to be customized for the response variable.
Basically we do some preprocessing of the data to get the baseline features
in a format that we will use of the analysis.
We reduce the number of race categories, simplify the educational attainment categories,
turn some groups into missing values and do other basic cleaning and partitioning.


```{r}
prepare_data_control <- function(data, agemin = 30, agemax = 80)
{
  base_df <- subset(data, RIDAGEYR > agemin & RIDAGEYR < agemax)
  base_df$race <- factor(base_df$RIDRETH1)
  levels(base_df$race) <- c("Hispanic+", "Black", "White", "Hispanic+", "Hispanic+")
  ## TODO: Similar for `DMDEDUC2`, maybe `INDFMPIR`
  nDEDUC = base_df$DMDEDUC2
  nDEDUC[nDEDUC == "Don't Know"] = NA
  nDEDUC[nDEDUC == "Refused"] = NA
  nDEDUC = factor(nDEDUC)
  levels(nDEDUC) = c("<HS", ">HS", "HS", "<HS", ">HS")
  base_df$DMDEDUC2 <- nDEDUC
  ## for poverty we will use a cutoff of 4 for INDFMPIR that gets close to the top 20% vs bottom 80%
  base_df$lowincome = base_df$INDFMPIR < 4
  ## for BMI let's just use obese vs everything else
  base_df$obese = base_df$BMXBMI >= 30
  base_df
}
prepare_data_bpsys <- function(data)
{
  bpsys_all <- data.matrix(data[c("BPXSY1", "BPXSY2", "BPXSY3", "BPXSY4")])
  bpsys_all[bpsys_all == 0] <- NA
  data$SYSTOLIC <- rowMeans(bpsys_all, na.rm = TRUE)
  data <- within(data,
                 {
                   INVSYSTOLIC <- 1 / SYSTOLIC
                   hbpMeds <- BPQ050A
                   hbpMeds[BPQ020 == "No"] <- "No"
                   hbpMeds[BPQ020 == "Don't know"] <- NA
                   hbpMeds[hbpMeds == "Don't know"] <- NA
                 })
  subset(data, is.finite(SYSTOLIC))
}
prepare_data <- function(CYCLE)
{
  print(CYCLE)
  cols <- list(DEMO = c("RIDAGEYR","RIAGENDR","RIDRETH1","DMDEDUC2","INDFMPIR"), 
               BPX  = c("BPXSY1", "BPXSY2", "BPXSY3", "BPXSY4"),
               BPQ  = c("BPQ050A","BPQ040A", "BPQ020","BPQ080","BPQ100D"),
               BMX  = "BMXBMI")
  names(cols) <- paste0(names(cols), CYCLE)
  base_df <- jointQuery(cols)
  ## extract relevant subset
  base_df <- prepare_data_control(base_df)
  base_df <- prepare_data_bpsys(base_df)
  keep_vars <-
    c("SYSTOLIC", "INVSYSTOLIC", "SEQN", "lowincome", "obese",
      "RIDAGEYR", "RIAGENDR", "race", "DMDEDUC2",
      "BeginYear", "EndYear")
  na.omit(base_df[keep_vars])
}
```

## Create the data set

Now once we have set up the baseline data we want to join that to the EnWAS variables and ensure that we have good data and that there are no missing values.
You could, at this point, spend some time imputing missing values or doing other basic model assessments.

```{r getNutrients}
mergeNutrients = function(CYCLE) {
  phenotypes = prepare_data(CYCLE)
  diet_table_name <- paste0("DR1TOT", CYCLE)
  diet_data <- as.data.frame(nhanes(diet_table_name))
  diet_data <- diet_data[exposure_vars]
  row.names(diet_data) <- diet_data$SEQN
  rownames(phenotypes) <- phenotypes$SEQN
  common_rows <- intersect(rownames(diet_data), rownames(phenotypes))
  df_merged <-
    cbind(phenotypes[common_rows, ], diet_data[common_rows, ]) |>
    subset(DR1DRSTZ == "Reliable and met the minimum criteria")
  na.omit(df_merged)
}

```

We might want a copy of the data all processed in order to do some different things, like a single omnibus analysis.  Or we might want to look at how food intake depends on some of the features we have put in our baseline model, like poverty, obesity etc.

```{r retrievealldata}

## first we merge the nutrients on a per cycle basis

cycles <- paste0("_", LETTERS[3:10])
alldata = lapply(cycles, mergeNutrients)

##make one big array and add a cycle variable - 
allD = do.call("rbind", alldata)
allD$Cycle = rep(cycles, times=sapply(alldata, nrow))


```

### Testing submodels

Now, we might want to ask what contribution each of the terms makes to the overall fit of the model.  We can use the likelihood ratio approach, or an F test to compare the sum of squared residuals from the fit with all variables in the model versus those with each one left out in turn.  The function `drop1` facilitates these computations. But, when the models contain interaction terms, as ours does, you need to do a little more work.
  In the output below we see the effect of removing each variable, while retaining the others. The row labeled `<none>` is the full model.  Note that we can drop the interaction term between age and sex, but this function does not report on what happens if either age or sex are themselves removed.  That is because it does not make any sense to retain the interaction term if we drop a main effect, and so we would need to drop 2 terms - but the function only supports dropping one.
   We can compute the appropriate tests by hand and then use the `anova` function to compare. From the call to `drop1` we can see that `Cycle` has the smallest F statistic, and that suggests that it is probably not having a big effect on our estimates. We note that the F statistic for leaving out age is close to double that of any other variable indicating the strength of the relationship between age and systolic blood pressure.  The reduction in residual sum of squares from including gender is actually quite small relative to the effects of removing other variables.

```{r basemodel}

combModel <- 'SYSTOLIC ~ ns(RIDAGEYR, df = 5) * RIAGENDR + race + obese + DMDEDUC2 + lowincome + Cycle'

lmC = lm(formula=combModel, data=allD)
summary(lmC)

drop1(lmC, test= "F")

noAgeM <- 'SYSTOLIC ~ RIAGENDR + race + obese + DMDEDUC2 + lowincome + Cycle'

lmnoA = lm(formula=noAgeM, data=allD)

anova(lmnoA, lmC)

noSexM = 'SYSTOLIC ~ ns(RIDAGEYR, df = 5) + race + obese + DMDEDUC2 + lowincome + Cycle'
lmnoS = lm(formula = noSexM, data = allD)

anova(lmnoS, lmC)

```

## Assess model fit

```{r modelFit}
##look at residuals from this model
# Example data

age_group <- cut(allD$RIDAGEYR, 
                 breaks = quantile(allD$RIDAGEYR, probs = seq(0, 1, length.out = 21), na.rm = TRUE), 
                 include.lowest = TRUE, 
                 labels = FALSE)

## here we can see a slight trend in increased variance that we might want to address
boxplot(lmC$residuals ~ age_group, main = "Large Model")

##not sure checking Cycle matters so much with it in the model - 
##TODO - is it worth fitting the model without cycle and showing the residuals?
# boxplot(lmC$residuals ~ allD$Cycle)
```



```{r enwasComb}

exposure_vars <-
    c("SEQN", "DR1DRSTZ", "DRDINT", "DRQSPREP","DR1TNUMF", "DR1TKCAL", "DR1TPROT",
      "DR1TCARB", "DR1TSUGR", "DR1TFIBE", "DR1TTFAT", "DR1TSFAT", "DR1TMFAT",
      "DR1TPFAT", "DR1TCHOL", "DR1TATOC", "DR1TRET"  ,"DR1TVARA", "DR1TBCAR",
      "DR1TCRYP", "DR1TLZ", "DR1TVB1" , "DR1TVB2" , "DR1TNIAC", "DR1TVB6" ,
      "DR1TFOLA", "DR1TFA", "DR1TFF", "DR1TFDFE", "DR1TVB12", "DR1TVC","DR1TVK",
      "DR1TCALC", "DR1TPHOS", "DR1TMAGN", "DR1TIRON", "DR1TZINC", "DR1TCOPP", "DR1TSODI",
      "DR1TPOTA", "DR1TSELE", "DR1TMOIS", "DR1TS040","DR1TS080", "DR1TS120",
      "DR1TS160", "DR1TS180", "DR1TM161", "DR1TM181", "DR1TM201", "DR1TP183",
      "DR1TP204")
 EVs <- exposure_vars[c(-1,-2,-3,-4)]
 
 ##now we have fit all of these.
  combDenwas = generic_enwas(data = allD, base_model = base_model, 
                expvars = EVs, trans = "invnorm")
  
 # colnames(xx)[c(1,6,7)] = c("estimate", "lower", "upper")
  
   xy = combDenwas |> dplyr::filter(LCL*UCL > 0) |>
    dplyr::top_n(10,abs(Estimate)) |> dplyr::arrange(dplyr::desc(Estimate))

     forestplot(labeltext = row.names(xy),
           mean = xy$Estimate,
           lower = xy$LCL,
           upper = xy$UCL,
           zero = 0, boxsize = 0.2,
           xlog = FALSE)
 
```

## Discussion of an alternative fitting approach

As noted above we can consider the EnWAS approach as a way to essentially *condition* on the risk factors and other features included in the base-line model. Then, for each exposure in turn we fit a separate model and then examine the size of the effect on outcome for that exposure.  An alternative to fitting the base-line model plus exposure is to instead first compute the residuals from the fit to the base-line model with no exposures and then use that as the response.  This simplifies the computations and if the exposures are orthogonal to the variables in the base-line model then one would actually get the same estimates.  This approach is widely used in GWAS and genetics applications mostly because the data sets are often in the hundreds of thousands to millions of observations and the computations are too expensive and time consuming.  We provide an option to do this and the plot below compares the estimates obtained for each exposure between re-fitting the regression each time to the approach that fits a regression to the residuals of the base-line model and the exposures. as you can see there is very little difference.

```{r}

 combD2 = generic_enwas(data = allD, base_model = base_model, 
                expvars = EVs, trans = "invnorm", useResiduals = TRUE)

 

 plot(combDenwas$Estimate, combD2$Estimate, main="Comparison of Two Fitting Approaches", 
      ylab = "Use Residuals", xlab  = "Refit Regression")          
     

```


```{r}
#| echo: FALSE

## here we likely need to spend some time evaluating the fit of the base models
evalBase <- function(df_merged)
{
  return(lm(formula = base_model, data = df_merged))
}


```

## A meta-analysis approach


### Fit the base model separately for each cycle and get a few summary stats from it

```{r fitbase}

eBase <- sapply(alldata, evalBase, simplify=FALSE)

sapply(eBase, function(x) summary(x)$r.squared)
sapply(eBase, function(x) summary(x)$coefficients["obeseTRUE",1])
sapply(eBase, function(x) summary(x)$coefficients["raceWhite",1])
sapply(eBase, function(x) summary(x)$coefficients["raceBlack",1])

```

### Now fit EnWAS to each cycle

We will just reuse the `alldata` object, which is a list where each element is the data set for one of the NHANES cycles.  We revert to the `base_model` which does not include a term for the cycle, since we are modeling each cycle separately.  And then we rearrange these data into a list where each element corresponds to one of the EnWAS variables and has the estimates from each of the cycles.

```{r}

e <- lapply(alldata, function(x) generic_enwas(x, base_model = base_model, 
                expvars = EVs, trans = "invnorm"))


byEV = vector("list", 48)
names(byEV) = row.names(e[[1]])

## could add in a row for the whole model estimate and one for the meta-analysis estimate

for(i in 1:48)
  byEV[[i]] = do.call("rbind", lapply(e, function(x) x[i,]))

## par(ask=TRUE) - to step through

```

## Compare estimates across cycles 

Now we can use the `metafor` package to do a meta-analysis for each of the EnWAS features.
Then, we can use that to identify some subse of them that might be interesting to create a forest plot.

```{r metafor}

#xx = byEV[[1]]
#xx$Var = xx$`Std. Error`^2

metaEV = lapply(byEV, \(x) {
        # Random-effects meta-analysis with clustering by cycle
         res <- rma.mv(yi = Estimate, 
              V = x$`Std. Error`^2, 
              random = ~ 1 | row.names(byEV[[1]]), 
              data = x)

         return(summary(res)) } )

##probably we should be looking at an FDR approach here
sort(sapply(metaEV, \(x) x$pval), dec=F)

```

Here we can create a forest plot for each of the different features. 

```{r tryfps}

mkFP = function(DATA, var) {
 xx = forestplot(labeltext = row.names(DATA),
           mean = DATA$Estimate,
           title = var,
           lower = DATA$LCL,
           upper = DATA$UCL,
           zero = 0, boxsize = 0.2,
           xlog = FALSE)
 return(xx)
}
# for(i in 1:48 ) plot(mkFP(byEV[[i]], names(byEV[i])))

mkFP(byEV[[1]], names(byEV[1]))


```






```{r}
getEstimate <- function(d) structure(d[["Estimate"]], names = d[["EV"]])
getTValue <- function(d) structure(d[["t value"]], names = d[["EV"]])
reorderRows <- function(x, FUN = mean, ...) x[order(apply(x, 1, FUN, ...)), ]
reorderCols <- function(x, FUN = mean, ...) x[, order(apply(x, 2, FUN, ...))]
```

```{r}
#| fig-width: 6
#| fig-height: 12
## sapply(e, getEstimate) |> heatmap()
sapply(e, getEstimate) |> reorderRows() |> reorderCols() |> t() |> levelplot()
```


```{r}
#| fig-width: 6
#| fig-height: 12
sapply(e, getTValue) |> reorderRows() |> reorderCols() |> t() |> levelplot()
```

## Explore relationship between risk factors and EnWAS features

Here we want to consider some tools that might help us understand what the relationship is between the risk factors included in the baseline model and the nutritional features included in the EnWAS.  If any of the nutritional features are highly related to a known risk factor then we would have some amount of confounding.


```{r rfforRace}
d1 = allD[, EVs]
d1 = apply(d1, 2, invNorm)

m1rf = randomForest(as.factor(allD$race) ~ ., data = d1)
varImpPlot(m1rf, n.var = 10, main="Predicting Race")

```

We will use machine learning and in particular Random Forests, since they are relatively easy to set up and are quite good methods for understanding prediction.
We look at predicting race and obesity. Show that the OOB error rates are pretty high, suggesting that there might not be a large amount of signal in the nutrient amounts regarding the risk factors we conditioned on.  We apply the inverse normal rank transformation to each exposure separately, since that is how they were fit in the EnWAS carried out above. When predicting race the out-of-bag error rate is `r round(m1rf$err.rate[nrow(m1rf$err.rate), "OOB"], digits=3)`.

```{r obeseRF}

##it seems randomForests doesn't treat logical vectors as factors...
m2rf = randomForest(as.factor(allD$obese) ~ ., data=d1)
varImpPlot(m2rf, n.var=10, main=" Predicting Obesity")
```

In this second example we predict obesity (BMI>30) and find the out-of-bag error rate is `r round(m2rf$err.rate[nrow(m1rf$err.rate), "OOB"], digits=3)`.
